{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_rag = pd.read_csv('results/naive_rag.csv', index_col=0)\n",
    "no_rag = pd.read_csv('results/no_rag.csv', index_col=0)\n",
    "prop_chunk_rag = pd.read_csv('results/prop_chunk_rag.csv', index_col=0)\n",
    "small2big_rag = pd.read_csv('results/small2big_rag.csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rouge(row):\n",
    "    reference = row['reference_answers']\n",
    "    prediction = row['generated_answers']\n",
    "\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    rouge_scores = scorer.score(reference, prediction)\n",
    "\n",
    "    return rouge_scores[\"rougeL\"].fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(df):\n",
    "    df[\"ROUGE-L\"] = df.apply(compute_rouge, axis=1, result_type=\"expand\")\n",
    "    print(f'Average ROUGE: {round(df['ROUGE-L'].mean(), 2)}')\n",
    "    print(f'Average Correctness: {df['correctness'].mean()}')\n",
    "    print(f'Average Groundedness: {df['groundedness'].mean()}')\n",
    "    print(f'Average Relevance: {df['relevance'].mean()}')\n",
    "    print(f'Average Retrieval Relevance: {df['retrieval_relevance'].mean()}')\n",
    "    print(f'Average Documents Length {df['documents'].apply(len).mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Correctness (Response vs reference answer)\n",
    "- Relevance (Response vs input)\n",
    "- Groundedness (Response vs retrieved docs)\n",
    "- Retrieval relevance (Retrieved docs vs input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No RAG\n",
      "Average ROUGE: 0.29\n",
      "Average Correctness: 1.56\n",
      "Average Groundedness: 1.08\n",
      "Average Relevance: 3.62\n",
      "Average Retrieval Relevance: 2.83\n",
      "Average Documents Length 8881.15\n",
      "Time taken to answer 100 questions: 9 minutes\n"
     ]
    }
   ],
   "source": [
    "print('No RAG')\n",
    "get_results(no_rag)\n",
    "print('Time taken to answer 100 questions: 9 minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive RAG\n",
      "Average ROUGE: 0.36\n",
      "Average Correctness: 2.14\n",
      "Average Groundedness: 1.23\n",
      "Average Relevance: 4.08\n",
      "Average Retrieval Relevance: 2.98\n",
      "Average Documents Length 8848.48\n",
      "Time taken to answer 100 questions: 14 minutes\n"
     ]
    }
   ],
   "source": [
    "print('Naive RAG')\n",
    "get_results(naive_rag)\n",
    "print('Time taken to answer 100 questions: 14 minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small2Big RAG\n",
      "Average ROUGE: 0.31\n",
      "Average Correctness: 2.08\n",
      "Average Groundedness: 1.12\n",
      "Average Relevance: 3.65\n",
      "Average Retrieval Relevance: 2.64\n",
      "Average Documents Length 13398.68\n",
      "Time taken to answer 100 questions: 21 minutes\n"
     ]
    }
   ],
   "source": [
    "print('Small2Big RAG')\n",
    "get_results(small2big_rag)\n",
    "print('Time taken to answer 100 questions: 21 minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proposition RAG\n",
      "Average ROUGE: 0.34\n",
      "Average Correctness: 1.88\n",
      "Average Groundedness: 1.81\n",
      "Average Relevance: 3.9\n",
      "Average Retrieval Relevance: 2.99\n",
      "Average Documents Length 346.45\n",
      "Time taken to answer 100 questions: 367 minutes\n"
     ]
    }
   ],
   "source": [
    "print('Proposition RAG')\n",
    "get_results(prop_chunk_rag)\n",
    "print('Time taken to answer 100 questions: 367 minutes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YmOSdh35HV9U"
   },
   "source": [
    "## Chunking techniques for Retrieval Augmented Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is ran on Kaggle (T4 x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T08:54:25.626991Z",
     "iopub.status.busy": "2025-01-05T08:54:25.626682Z",
     "iopub.status.idle": "2025-01-05T08:54:46.962176Z",
     "shell.execute_reply": "2025-01-05T08:54:46.961358Z",
     "shell.execute_reply.started": "2025-01-05T08:54:25.626955Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir llama-cpp-python==0.2.90 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
    "!pip install -qU json-repair datasets langchain==0.3.13 langchain-community==0.3.13 autoawq transformers accelerate faiss-gpu wikipedia googlesearch-python\n",
    "!git clone https://github.com/chiphuyen/lazynlp.git\n",
    "!pip install -r /kaggle/working/lazynlp/requirements.txt\n",
    "\n",
    "import sys\n",
    "sys.path.append('/kaggle/working/lazynlp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPjNaM6KHV9V"
   },
   "source": [
    "## RAG Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHPTVw2tHV9W"
   },
   "source": [
    "### 1. Chat model\n",
    "Qwen2.5-7B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T08:55:36.833792Z",
     "iopub.status.busy": "2025-01-05T08:55:36.833512Z",
     "iopub.status.idle": "2025-01-05T08:56:39.405350Z",
     "shell.execute_reply": "2025-01-05T08:56:39.404389Z",
     "shell.execute_reply.started": "2025-01-05T08:55:36.833770Z"
    },
    "id": "vW_AqT4Fh39A",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatLlamaCpp\n",
    "\n",
    "chat_model = ChatLlamaCpp(\n",
    "    model_path='/kaggle/input/rag-system/gguf/default/2/Qwen2.5-7B-Instruct.Q5_K_S.gguf',\n",
    "    temperature=0.3,\n",
    "    max_tokens=512,\n",
    "    n_batch=32,\n",
    "    n_ctx=32768,\n",
    "    n_gpu_layers=-1,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChBcP3eCHV9Y"
   },
   "source": [
    "### 2. Embedding model\n",
    "mxbai-embed-large-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T08:56:39.406690Z",
     "iopub.status.busy": "2025-01-05T08:56:39.406349Z",
     "iopub.status.idle": "2025-01-05T08:56:42.523564Z",
     "shell.execute_reply": "2025-01-05T08:56:42.522886Z",
     "shell.execute_reply.started": "2025-01-05T08:56:39.406670Z"
    },
    "id": "3wllGgkZHV9Z",
    "outputId": "814a9b1b-022a-4f67-e01a-f135d739b070",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import LlamaCppEmbeddings\n",
    "\n",
    "embeddings = LlamaCppEmbeddings(\n",
    "    model_path=\"/kaggle/input/rag-system/gguf/default/2/mxbai-embed-large-v1.Q5_K_M.gguf\",\n",
    "    device='cuda',\n",
    "    n_gpu_layers=-1,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPoGlH_JwoEF"
   },
   "source": [
    "### 3. Documents retriever and Vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve documents through Google searches and Wikipedia articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T08:56:42.524891Z",
     "iopub.status.busy": "2025-01-05T08:56:42.524505Z",
     "iopub.status.idle": "2025-01-05T08:56:42.920472Z",
     "shell.execute_reply": "2025-01-05T08:56:42.919541Z",
     "shell.execute_reply.started": "2025-01-05T08:56:42.524831Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "from googlesearch import search\n",
    "from langchain_core.documents import Document\n",
    "import lazynlp\n",
    "import urllib\n",
    "# headers for scraping\n",
    "headers = {        \n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',         \n",
    "        'Accept-Language': 'en-US,en;q=0.5',        \n",
    "        'Connection': 'keep-alive',                         \n",
    "        'Referer': 'https://search.brave.com/',\n",
    "        'Sec-Fetch-Dest': 'document',        \n",
    "        'Sec-Fetch-Mode': 'navigate',        \n",
    "        'Sec-Fetch-Site': 'none',        \n",
    "        'Sec-Fetch-User': '?1',        \n",
    "        'TE': 'trailers',   \n",
    "        'Upgrade-Insecure-Requests': '1',        \n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:133.0) Gecko/20100101 Firefox/133.0'        \n",
    "    }\n",
    "\n",
    "class CustomWebRetriever():\n",
    "    \"\"\"\n",
    "    Given a query, retrieve top 5 Google searches and top 2 Wikipedia articles.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.wiki_retriever = WikipediaRetriever(top_k_results=2)\n",
    "\n",
    "    def _get_documents_from_urls(self, urls):\n",
    "        \"\"\"\n",
    "        1. Get htmls for each urls.\n",
    "        2. Get page contents from htmls using lazynlp.\n",
    "        3. If url request failed (4xx) or cannot get page content, skip that url.\n",
    "        4. Return a list of documents.\n",
    "        \"\"\"\n",
    "        docs = []\n",
    "        for url in urls:\n",
    "            try:\n",
    "                req = urllib.request.Request(url, headers=headers)\n",
    "                response = urllib.request.urlopen(req, timeout=3)\n",
    "                page = response.read()\n",
    "\n",
    "                cleaned_page = lazynlp.clean_page(page)\n",
    "                if cleaned_page:\n",
    "                    docs.append(Document(page_content=cleaned_page))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return docs\n",
    "    \n",
    "    def invoke(self, query):\n",
    "        \"\"\"\n",
    "        Retrieve list of relevant documents based on query.\n",
    "        \"\"\"\n",
    "        wiki_docs = self.wiki_retriever.invoke(query)\n",
    "        urls = [url for url in search(query, num_results=5) if 'wikipedia.org' not in url]\n",
    "    \n",
    "        web_docs = self._get_documents_from_urls(urls)\n",
    "        web_docs.extend(wiki_docs)\n",
    "        for doc in web_docs:\n",
    "            doc.page_content = doc.page_content.replace('\\t', ' ')\n",
    "            doc.page_content = doc.page_content.replace('\\n', ' ')\n",
    "            doc.page_content = ' '.join(doc.page_content.split())\n",
    "\n",
    "            \n",
    "        return web_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T08:56:42.921926Z",
     "iopub.status.busy": "2025-01-05T08:56:42.921450Z",
     "iopub.status.idle": "2025-01-05T08:56:42.952222Z",
     "shell.execute_reply": "2025-01-05T08:56:42.951428Z",
     "shell.execute_reply.started": "2025-01-05T08:56:42.921895Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "class BaseVectorstoreRetriever():\n",
    "    \"\"\"\n",
    "    Base class for using FAISS vectorstore to store documents (chunks), and retrieve context for chat model given query.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        index = faiss.IndexFlatL2(len(embeddings.embed_query(\"hello world\")))\n",
    "        self.vector_store = FAISS(\n",
    "            embedding_function=embeddings,\n",
    "            index=index,\n",
    "            docstore= InMemoryDocstore(),\n",
    "            index_to_docstore_id={}\n",
    "        )\n",
    "        self.retriever = CustomWebRetriever()\n",
    "\n",
    "    def _get_docs_list(self, docs):\n",
    "        return\n",
    "    \n",
    "    def invoke(self, query):\n",
    "        \"\"\"\n",
    "        1. Retrieve list of relevant documents (chunks) based on query.\n",
    "        2. Add documents (chunks) to vectorstore.\n",
    "        3. Retrieve list of contexts most similar (top 5) to query.\n",
    "        \"\"\"\n",
    "        docs = self.retriever.invoke(query)\n",
    "        docs_list = self._get_docs_list(docs)\n",
    "\n",
    "        self.vector_store.add_documents(documents=docs_list)\n",
    "        \n",
    "        vectorstore_retriever = self.vector_store.as_retriever(search_type=\"similarity\", search_kwargs={'k': 5})\n",
    "        return vectorstore_retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYnAm1ySXlQr"
   },
   "source": [
    "Naive Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T08:56:42.953252Z",
     "iopub.status.busy": "2025-01-05T08:56:42.953033Z",
     "iopub.status.idle": "2025-01-05T08:56:42.964105Z",
     "shell.execute_reply": "2025-01-05T08:56:42.963409Z",
     "shell.execute_reply.started": "2025-01-05T08:56:42.953223Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "class NaiveVectorstoreRetriever(BaseVectorstoreRetriever):\n",
    "    \"\"\"\n",
    "    Split documents into chunks with each chunk size = 2000 and overlap between chunks = 200.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200, length_function=len)\n",
    "\n",
    "    def _get_docs_list(self, docs):\n",
    "        return self.text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Propositions Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T08:56:42.966824Z",
     "iopub.status.busy": "2025-01-05T08:56:42.966627Z",
     "iopub.status.idle": "2025-01-05T08:56:48.272649Z",
     "shell.execute_reply": "2025-01-05T08:56:48.271962Z",
     "shell.execute_reply.started": "2025-01-05T08:56:42.966807Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from json_repair import repair_json\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "class PropositionsChunkingVectorstoreRetriever(BaseVectorstoreRetriever):\n",
    "    \"\"\"\n",
    "    Split documents based on propositions (self-contained facts) and fall back on sentence chunking.\n",
    "    Propositionizer model from paper 'Dense X Retrieval: What Retrieval Granularity Should We Use?'.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model_name = \"chentong00/propositionizer-wiki-flan-t5-large\"\n",
    "        self.device = \"cuda\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n",
    "\n",
    "    def _generate_propositions(self, title, chunk, propositions):\n",
    "        \"\"\"\n",
    "        1. Generate propositions from each chunk (list of sentences) using propositionizer model.\n",
    "        2. Get/fix json string from model output to get list of propositions.\n",
    "        3. If json string cannot be fix then fall back to using the original chunk (list of sentences).\n",
    "        4. Return list of documents that are propositions/sentence chunking.\n",
    "        \"\"\"\n",
    "        input_text = f\"Title: {title}. Section: . Content: {' '.join(chunk)}\"\n",
    "        input_ids = self.tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "        outputs = self.model.generate(input_ids.to(self.device), max_new_tokens=512).cpu()\n",
    "        output_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        fixed_output_text = repair_json(output_text, ensure_ascii=False)\n",
    "\n",
    "        if fixed_output_text == '':\n",
    "            prop_doc_list = [Document(page_content=text, title=title) for text in chunk]\n",
    "        else:\n",
    "            prop_list = json.loads(fixed_output_text)\n",
    "            prop_doc_list = [Document(page_content=text, title=title) for text in set(prop_list) if isinstance(text, str)]\n",
    "        propositions.extend(prop_doc_list)\n",
    "\n",
    "        return propositions\n",
    "    \n",
    "    def _get_propositions_with_overlap(self, docs_list, max_chunk_tokens=450):\n",
    "        \"\"\"\n",
    "        1. Split document into sentences using nltk\n",
    "        2. Concat sentences into a chunk until chunk tokens reach max chunk tokens of 450\n",
    "        3. One sentence overlap between chunks\n",
    "        \"\"\"\n",
    "        propositions = []\n",
    "        \n",
    "        for doc in docs_list:\n",
    "            cleaned_context = doc.page_content.replace(\"\\n\", \" \")\n",
    "            sentences = sent_tokenize(cleaned_context)\n",
    "            chunk = []\n",
    "            current_tokens = 0\n",
    "            last_sentence = \"\"\n",
    "            doc_title = doc.metadata.get('title', '')\n",
    "            title_length = len(doc_title)\n",
    "            for sentence in dict.fromkeys(sentences):\n",
    "                token_count = len(self.tokenizer.encode(sentence, truncation=False))\n",
    "                if current_tokens + token_count + title_length <= max_chunk_tokens:\n",
    "                    chunk.append(sentence)\n",
    "                    current_tokens += token_count\n",
    "                else:\n",
    "                    propositions = self._generate_propositions(doc_title, chunk, propositions)\n",
    "                    chunk = [last_sentence, sentence]\n",
    "                    current_tokens = len(self.tokenizer.encode(' '.join(chunk), truncation=False))\n",
    "    \n",
    "                last_sentence = sentence\n",
    "    \n",
    "            if chunk:\n",
    "                propositions = self._generate_propositions(doc_title, chunk, propositions)\n",
    "    \n",
    "        return propositions\n",
    "\n",
    "    def _get_docs_list(self, docs):\n",
    "        return self._get_propositions_with_overlap(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small2Big Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T08:56:48.274125Z",
     "iopub.status.busy": "2025-01-05T08:56:48.273733Z",
     "iopub.status.idle": "2025-01-05T08:56:48.407013Z",
     "shell.execute_reply": "2025-01-05T08:56:48.406388Z",
     "shell.execute_reply.started": "2025-01-05T08:56:48.274105Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever, SearchType\n",
    "\n",
    "class Small2BigVectorstoreRetriever(BaseVectorstoreRetriever):\n",
    "    \"\"\"\n",
    "    Embed small chunks for better semantic similarity while retrieving parent chunks for better context.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Parent chunks with length of 10000 and overlap of 1000.\n",
    "\n",
    "        Child chunks with length of 400, chunk_overlap = 40.\n",
    "        \n",
    "        Parent documents retriever with associated identifier, search with maximal marginal relevance.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.parent_text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000, length_function=len)\n",
    "        self.child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=40, length_function=len)\n",
    "        self.id_key = \"doc_id\"\n",
    "        self.parent_retriever = MultiVectorRetriever(\n",
    "            vectorstore=self.vector_store,\n",
    "            byte_store=InMemoryByteStore(),\n",
    "            id_key=self.id_key,\n",
    "        )\n",
    "        self.parent_retriever.search_type = SearchType.mmr\n",
    "        \n",
    "    def invoke(self, query):\n",
    "        \"\"\"\n",
    "        1. Retrieve and split into parent chunks.\n",
    "        2. For each parent chunks split into child chunks.\n",
    "        3. Set the id for the child chunks as the id of the parent document.\n",
    "        4. Embed child chunks into vectorstore.\n",
    "        5. The retriever will find child chunks with the closest (mmr) embedding to the query, and will retrive the\n",
    "        parent document via id.\n",
    "        \"\"\"\n",
    "        docs = self.retriever.invoke(query)\n",
    "        parent_docs = self.parent_text_splitter.split_documents(docs)\n",
    "        parent_doc_ids = [str(uuid.uuid4()) for _ in parent_docs]\n",
    "\n",
    "        sub_docs = []\n",
    "        for i, doc in enumerate(parent_docs):\n",
    "            _id = parent_doc_ids[i]\n",
    "            _sub_docs = self.child_text_splitter.split_documents([doc])\n",
    "            for _doc in _sub_docs:\n",
    "                _doc.metadata[self.id_key] = _id\n",
    "            sub_docs.extend(_sub_docs)\n",
    "\n",
    "        self.parent_retriever.vectorstore.add_documents(sub_docs)\n",
    "        self.parent_retriever.docstore.mset(list(zip(parent_doc_ids, docs)))\n",
    "\n",
    "        return self.parent_retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0zmPZkREuBu"
   },
   "source": [
    "### 4. Context retrieval and Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T08:56:48.408104Z",
     "iopub.status.busy": "2025-01-05T08:56:48.407869Z",
     "iopub.status.idle": "2025-01-05T08:56:48.553701Z",
     "shell.execute_reply": "2025-01-05T08:56:48.552792Z",
     "shell.execute_reply.started": "2025-01-05T08:56:48.408084Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def rag_bot(\n",
    "    questions: list[str], \n",
    "    retriever=NaiveVectorstoreRetriever(), \n",
    "    rag=True\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Retrieval Augmented Chatbot (Q&A). Can pass in multiple questions.\n",
    "    \n",
    "    Default retriever is Naive (rag=True), set rag=False to not use the RAG System.\n",
    "    \n",
    "    Returns answers, documents retrieved and run time (s).\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    batch_docs = [retriever.invoke(question) for question in questions]\n",
    " \n",
    "    instructions_batch = []\n",
    "    for question, docs in zip(questions, batch_docs):\n",
    "        if rag:\n",
    "            docs_string = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "            # RAG Prompt\n",
    "            instructions = f\"\"\"You are a helpful assistant who provides concise answers. \n",
    "Use the following source documents to answer the user's questions. \n",
    "\n",
    "Documents:\n",
    "{docs_string}\n",
    "\n",
    "That's the end of documents. Answer the question with either a single word/phrase.\n",
    "If you don't know the answer, just say that you don't know.\"\"\"\n",
    "        else:\n",
    "            # Normal Chatbot Prompt\n",
    "            instructions = \"\"\"You are a helpful assistant who provides concise answers. \n",
    "Answer the question with either a single word/phrase.\n",
    "If you don't know the answer, just say that you don't know.\"\"\"\n",
    "        \n",
    "        instructions_batch.append(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": instructions},\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    ai_responses = [chat_model.invoke(messages) for messages in instructions_batch]\n",
    "\n",
    "    results = [\n",
    "        {\"answer\": ai_msg.content, \"documents\": docs}\n",
    "        for ai_msg, docs in zip(ai_responses, batch_docs)\n",
    "    ]\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    return results, elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3p0-kO5uNC0"
   },
   "source": [
    "### 5. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation using LLM-as-a-judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T08:56:48.554929Z",
     "iopub.status.busy": "2025-01-05T08:56:48.554672Z",
     "iopub.status.idle": "2025-01-05T08:58:41.760125Z",
     "shell.execute_reply": "2025-01-05T08:58:41.759398Z",
     "shell.execute_reply.started": "2025-01-05T08:56:48.554905Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "judge_tokenizer = AutoTokenizer.from_pretrained(\"PrunaAI/prometheus-eval-prometheus-7b-v2.0-AWQ-4bit-smashed\")\n",
    "judge_model = AutoModelForCausalLM.from_pretrained(\"PrunaAI/prometheus-eval-prometheus-7b-v2.0-AWQ-4bit-smashed\", device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T08:58:41.761544Z",
     "iopub.status.busy": "2025-01-05T08:58:41.760975Z",
     "iopub.status.idle": "2025-01-05T08:58:41.765700Z",
     "shell.execute_reply": "2025-01-05T08:58:41.764811Z",
     "shell.execute_reply.started": "2025-01-05T08:58:41.761510Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def get_result(text):\n",
    "    \"\"\"\n",
    "    Parse rubric score from judge model's output.\n",
    "     \n",
    "    If the output is not in the right format and cannot be parsed, return -1.\n",
    "    \"\"\"\n",
    "    result_match = re.search(r\"\\[RESULT\\]\\s*(\\d+)\", text) \n",
    "    if result_match is None:\n",
    "        return -1\n",
    "    result = int(result_match.group(1))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T08:58:41.766750Z",
     "iopub.status.busy": "2025-01-05T08:58:41.766472Z",
     "iopub.status.idle": "2025-01-05T08:58:41.784856Z",
     "shell.execute_reply": "2025-01-05T08:58:41.783911Z",
     "shell.execute_reply.started": "2025-01-05T08:58:41.766716Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SCORE_RUBRIC_TEMPLATE = \"\"\"\n",
    "[{criteria}]\n",
    "Score 1: {score1_description}\n",
    "Score 2: {score2_description}\n",
    "Score 3: {score3_description}\n",
    "Score 4: {score4_description}\n",
    "Score 5: {score5_description}\n",
    "\"\"\".strip()\n",
    "\n",
    "ABS_SYSTEM_PROMPT = \"You are a fair judge assistant tasked with providing clear, objective feedback based on specific criteria, ensuring each assessment reflects the absolute standards set for performance.\"\n",
    "\n",
    "ABSOLUTE_PROMPT_WO_REF = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \"(write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\"\n",
    "4. Please do not generate any other opening, closing, and explanations.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Score Rubrics:\n",
    "{rubric}\n",
    "\n",
    "###Feedback: \"\"\"\n",
    "\n",
    "def evaluate(instructions, responses, rubric_data):\n",
    "    \"\"\"\n",
    "    1. Get correct instruction for each evaluation factor.\n",
    "    2. Get the judge model output (feedback and score) of the evaluation between the responses and the instructions.\n",
    "    3. Return the judge model score, -1 if score cannot be parsed from judge model output.\n",
    "    \"\"\"\n",
    "    batch_messages = []\n",
    "    for instruction, response in zip(instructions, responses):\n",
    "        user_content = ABS_SYSTEM_PROMPT + \"\\n\\n\" + ABSOLUTE_PROMPT_WO_REF.format(\n",
    "            instruction=instruction,\n",
    "            response=response,\n",
    "            rubric=SCORE_RUBRIC_TEMPLATE.format(**rubric_data),\n",
    "        )\n",
    "        batch_messages.append([{\"role\": \"user\", \"content\": user_content}])\n",
    "        \n",
    "    tokenized_inputs = [\n",
    "        judge_tokenizer.apply_chat_template([message], return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "        for message in batch_messages\n",
    "    ]\n",
    "\n",
    "    attention_masks = [\n",
    "        (inputs != judge_tokenizer.pad_token_id).int().to(\"cuda\")\n",
    "        for inputs in tokenized_inputs\n",
    "    ]\n",
    "    \n",
    "    generated_ids = [\n",
    "        judge_model.generate(\n",
    "            inputs,\n",
    "            attention_mask=mask,\n",
    "            max_new_tokens=1000,\n",
    "            do_sample=False,\n",
    "            pad_token_id=judge_tokenizer.eos_token_id\n",
    "        )\n",
    "        for inputs, mask in zip(tokenized_inputs, attention_masks)\n",
    "    ]\n",
    "    \n",
    "    decoded_texts = [\n",
    "        judge_tokenizer.decode(ids[0], skip_special_tokens=True)\n",
    "        for ids in generated_ids\n",
    "    ]\n",
    "    \n",
    "    return [get_result(output) for output in decoded_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metrics:\n",
    "- Correctness (Response vs reference answer)\n",
    "- Relevance (Response vs input)\n",
    "- Groundedness (Response vs retrieved docs)\n",
    "- Retrieval relevance (Retrieved docs vs input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T08:58:41.785880Z",
     "iopub.status.busy": "2025-01-05T08:58:41.785631Z",
     "iopub.status.idle": "2025-01-05T08:58:41.804100Z",
     "shell.execute_reply": "2025-01-05T08:58:41.803320Z",
     "shell.execute_reply.started": "2025-01-05T08:58:41.785848Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "correctness_rubric_data = {\n",
    "    \"criteria\": \"How similar/correct is the model response, relative to the reference answer?\",\n",
    "    \"score1_description\": \"The response is largely incorrect or diverges significantly from the reference answer, offering little to no overlap or relevance.\",\n",
    "    \"score2_description\": \"The response contains some elements that align with the reference answer but misses key details or includes substantial inaccuracies.\",\n",
    "    \"score3_description\": \"The response covers the main points of the reference answer but may omit minor details or introduce minor inaccuracies.\",\n",
    "    \"score4_description\": \"The response aligns well with the reference answer, accurately capturing most details with rare and insignificant deviations.\",\n",
    "    \"score5_description\": \"The response is highly accurate, fully aligned with the reference answer, and captures all key points without any inaccuracies or omissions.\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T08:58:41.805150Z",
     "iopub.status.busy": "2025-01-05T08:58:41.804951Z",
     "iopub.status.idle": "2025-01-05T08:58:41.818034Z",
     "shell.execute_reply": "2025-01-05T08:58:41.817333Z",
     "shell.execute_reply.started": "2025-01-05T08:58:41.805133Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "relevance_rubric_data = {\n",
    "    \"criteria\": \"How well does the generated response address the initial user input?\",\n",
    "    \"score1_description\": \"The response is completely off-topic or fails to address the input in any meaningful way.\",\n",
    "    \"score2_description\": \"The response has some relevance to the input but fails to address it fully or includes unrelated information.\",\n",
    "    \"score3_description\": \"The response is generally relevant to the input, though it may miss certain aspects or include unnecessary information.\",\n",
    "    \"score4_description\": \"The response is closely aligned with the input, addressing it effectively with minimal extraneous content.\",\n",
    "    \"score5_description\": \"The response is fully relevant, directly and comprehensively addressing the input in a precise and focused manner.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T08:58:41.818984Z",
     "iopub.status.busy": "2025-01-05T08:58:41.818716Z",
     "iopub.status.idle": "2025-01-05T08:58:41.833791Z",
     "shell.execute_reply": "2025-01-05T08:58:41.833157Z",
     "shell.execute_reply.started": "2025-01-05T08:58:41.818962Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "groundedness_rubric_data = {\n",
    "    \"criteria\": \"To what extent does the generated response agree with the retrieved context?\",\n",
    "    \"score1_description\": \"The response is largely ungrounded, containing significant deviations or contradictions from the retrieved context.\",\n",
    "    \"score2_description\": \"The response occasionally references the retrieved context but includes several inaccuracies or unsupported claims.\",\n",
    "    \"score3_description\": \"The response is mostly grounded in the retrieved context but may omit key points or introduce minor unsupported details.\",\n",
    "    \"score4_description\": \"The response is well-grounded in the retrieved context, with rare and minor inaccuracies or omissions.\",\n",
    "    \"score5_description\": \"The response is fully grounded, accurately and comprehensively reflecting the retrieved context without any inconsistencies.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T08:58:41.834996Z",
     "iopub.status.busy": "2025-01-05T08:58:41.834705Z",
     "iopub.status.idle": "2025-01-05T08:58:41.852406Z",
     "shell.execute_reply": "2025-01-05T08:58:41.851767Z",
     "shell.execute_reply.started": "2025-01-05T08:58:41.834965Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "retrieval_relevance_rubric_data = {\n",
    "\"criteria\": \"How relevant are the retrieved documents to the input query?\",\n",
    "\"score1_description\": \"The retrieved documents are irrelevant to the input query, offering no meaningful connection or support.\",\n",
    "\"score2_description\": \"The retrieved documents have minimal relevance, with only a small fraction addressing the query appropriately.\",\n",
    "\"score3_description\": \"The retrieved documents are moderately relevant, partially addressing the query but missing some key aspects.\",\n",
    "\"score4_description\": \"The retrieved documents are highly relevant, addressing the query effectively with only minor gaps or unrelated content.\",\n",
    "\"score5_description\": \"The retrieved documents are perfectly relevant, fully addressing the input query comprehensively and precisely.\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T08:58:41.853497Z",
     "iopub.status.busy": "2025-01-05T08:58:41.853235Z",
     "iopub.status.idle": "2025-01-05T08:58:44.105579Z",
     "shell.execute_reply": "2025-01-05T08:58:44.104910Z",
     "shell.execute_reply.started": "2025-01-05T08:58:41.853459Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "SAMPLE = 100\n",
    "ds = load_dataset(\"Stanford/web_questions\", split='test')\n",
    "# Only consider sample with ONE ground truth answer\n",
    "ds = ds.filter(lambda example: len(example['answers'])==1)\n",
    "ds = ds.shuffle(seed=24)\n",
    "# Get 100 samples\n",
    "ds = Dataset.from_dict(ds[:SAMPLE])\n",
    "\n",
    "questions = ds['question']\n",
    "\n",
    "reference_answers = [a[0] for a in ds['answers']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-05T09:04:09.612Z",
     "iopub.execute_input": "2025-01-05T08:58:44.106587Z",
     "iopub.status.busy": "2025-01-05T08:58:44.106383Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Change retriver for each retriever method (or set rag=False for base chat model)\n",
    "outputs, total_time = rag_bot(ds['question'], retriever = PropositionsChunkingVectorstoreRetriever())\n",
    "\n",
    "answers = [output['answer'] for output in outputs]\n",
    "\n",
    "documents = [\"\\n\\n\".join(doc.page_content for doc in output['documents']) for output in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-05T09:04:09.612Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "del chat_model\n",
    "del embeddings\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Correctness (Response vs reference answer)\n",
    "- Relevance (Response vs input)\n",
    "- Groundedness (Response vs retrieved docs)\n",
    "- Retrieval relevance (Retrieved docs vs input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-05T09:04:09.612Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "correctness_score = evaluate(\n",
    "    instructions = reference_answers, \n",
    "    responses = answers,\n",
    "    rubric_data = correctness_rubric_data, \n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "relevance_score = evaluate(\n",
    "    instructions = questions, \n",
    "    responses = answers, \n",
    "    rubric_data = relevance_rubric_data, \n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "groundedness_score = evaluate(\n",
    "    instructions = documents, \n",
    "    responses = answers,\n",
    "    rubric_data = groundedness_rubric_data, \n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "retrieval_relevance_score = evaluate(\n",
    "    instructions = questions, \n",
    "    responses = documents,\n",
    "    rubric_data = retrieval_relevance_rubric_data, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-05T09:04:09.612Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Change file name for each retriever method\n",
    "pd.DataFrame({\n",
    "    'questions': questions,\n",
    "    'generated_answers': answers,\n",
    "    'reference_answers': reference_answers,\n",
    "    'documents': documents,\n",
    "    'correctness' : correctness_score,\n",
    "    'groundedness' : groundedness_score,\n",
    "    'relevance' : relevance_score,\n",
    "    'retrieval_relevance' : retrieval_relevance_score,\n",
    "}).to_csv('prop_chunk_rag.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6385064,
     "sourceId": 10313894,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6405373,
     "sourceId": 10343784,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6406543,
     "sourceId": 10346011,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 202815,
     "modelInstanceId": 180557,
     "sourceId": 216633,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "034edf9e344f40b9bbd7b424766e634a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5ab0600a34754cf48ab575acf0a2ff04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6e8ef52bf06c45ccb9f46a1fafee3192": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d355275d18254e89beae8484a8cba913",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f73b1583c8ad4c74b173c217f00bf3d5",
      "value": 1
     }
    },
    "90b4fd93e12b4aa885829e2e5ede6cee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b90e41c435674aea913c2167bb24feba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cba9fc69d8e0458b87c10a52431f0e06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_90b4fd93e12b4aa885829e2e5ede6cee",
      "placeholder": "​",
      "style": "IPY_MODEL_b90e41c435674aea913c2167bb24feba",
      "value": ""
     }
    },
    "cc67f555bcb64d78a79871e7ba08e295": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5ab0600a34754cf48ab575acf0a2ff04",
      "placeholder": "​",
      "style": "IPY_MODEL_034edf9e344f40b9bbd7b424766e634a",
      "value": " 84/? [06:54&lt;00:00,  4.73s/it]"
     }
    },
    "d355275d18254e89beae8484a8cba913": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "d67ab37b6dc74a9cbe87a0dcd2a7d840": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cba9fc69d8e0458b87c10a52431f0e06",
       "IPY_MODEL_6e8ef52bf06c45ccb9f46a1fafee3192",
       "IPY_MODEL_cc67f555bcb64d78a79871e7ba08e295"
      ],
      "layout": "IPY_MODEL_f13145a810a1411bbfcee1d4d891dfce"
     }
    },
    "f13145a810a1411bbfcee1d4d891dfce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f73b1583c8ad4c74b173c217f00bf3d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
